{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasoning Agent HITL Verification\n",
    "\n",
    "This notebook verifies **Stage 3 Step 3: The Code/Reasoning Agent**.\n",
    "\n",
    "### Verification Goals:\n",
    "1. **Deterministic Math:** Does it use Python to calculate deltas or percentages?\n",
    "2. **Self-Awareness:** Does it warn the user if data is missing or if it's guessing?\n",
    "3. **Nuance & Citations:** Does it cite `row_id` and `chunk_id` correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "from venra.navigator import Navigator\n",
    "from venra.retriever import DualRetriever\n",
    "from venra.assembler import ContextAssembler\n",
    "from venra.agent import ReasoningAgent\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-03 17:52:42,287 - venra - INFO - Retriever loaded UFL with 253 rows.\n"
     ]
    }
   ],
   "source": [
    "PDF_PATH = \"../data/10K_TD_test.pdf\"\n",
    "file_prefix = os.path.basename(PDF_PATH).replace(\".pdf\", \"\")\n",
    "\n",
    "nav = Navigator(file_prefix=file_prefix)\n",
    "retriever = DualRetriever(file_prefix=file_prefix)\n",
    "assembler = ContextAssembler()\n",
    "agent = ReasoningAgent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Complex Query (Math + Text)\n",
    "\n",
    "\"How much the sale excluding acquisition increase compare to last year and what it was due to?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-03 17:38:11,716 - venra - INFO - Navigating query: How much the sale excluding acquisition increase compare to last year and what it was due to?\n",
      "2026-02-03 17:38:12,459 - venra - INFO - Plan generated. Reasoning: The user is asking for the increase in Net Sales excluding Acquisition Sales compared to last year and what it was due to. This requires looking at the Net Sales and Acquisition Sales metrics for both 2025 and 2024, and then calculating the increase in Net Sales excluding Acquisition Sales. The nuance focus is null, indicating that the user is looking for the raw data without any adjustments.\n",
      "2026-02-03 17:38:12,459 - venra - INFO - Starting retrieval for query: Net Sales excluding Acquisition Sales Increase Com... (k=3)\n",
      "2026-02-03 17:38:13,390 - venra - INFO - Retrieval complete: 22 UFL rows, 5 text chunks.\n",
      "2026-02-03 17:38:13,402 - venra - INFO - Agent processing query (Pass 1: Kimi Reasoning): How much the sale excluding acquisition increase compare to last year and what it was due to?\n",
      "2026-02-03 17:39:39,628 - venra - INFO - Reasoning Plan: Step 1: Identify that 'sales excluding acquisition' refers to Organic Sales, as defined in text chunk a6ddc782-e158-45cf-9027-bddf4262cdde ('Organic sales represent net sales from existing businesses... excluding sales from acquisitions'). Step 2: Extract the dollar increase in organic sales for 2025 vs 2024. The UFL row 931d9581c4416e15b8ad17b1cc70d856 shows 6.15e+08 USD ($615 million), which is confirmed by text chunk a6ddc782... ('increase in organic sales of $615 million') and the table in chunk 0dbf27f3... (showing $615). Step 3: Note the percentage increase is 7.7% from the table in chunk 0dbf27f3... Step 4: Identify the drivers from text chunk a6ddc782... ('primarily related to increases in defense and commercial aftermarket') and detailed segment breakdowns from chunk 314fe7fb... (Power & Control: defense, commercial aftermarket, commercial OEM; Airframe: defense and commercial aftermarket). Step 5: Explicitly disregard UFL row 0f9c141a4623219910b7c3d78b3d3704 which incorrectly lists 2.52e+08 (this is the Cost of Sales increase, not Organic Sales, despite the label).\n",
      "2026-02-03 17:39:39,630 - venra - INFO - Pass 2: Synthesis (Fast)\n",
      "=== AGENT THOUGHT PROCESS ===\n",
      "PLAN: Step 1: Identify that 'sales excluding acquisition' refers to Organic Sales, as defined in text chunk a6ddc782-e158-45cf-9027-bddf4262cdde ('Organic sales represent net sales from existing businesses... excluding sales from acquisitions'). Step 2: Extract the dollar increase in organic sales for 2025 vs 2024. The UFL row 931d9581c4416e15b8ad17b1cc70d856 shows 6.15e+08 USD ($615 million), which is confirmed by text chunk a6ddc782... ('increase in organic sales of $615 million') and the table in chunk 0dbf27f3... (showing $615). Step 3: Note the percentage increase is 7.7% from the table in chunk 0dbf27f3... Step 4: Identify the drivers from text chunk a6ddc782... ('primarily related to increases in defense and commercial aftermarket') and detailed segment breakdowns from chunk 314fe7fb... (Power & Control: defense, commercial aftermarket, commercial OEM; Airframe: defense and commercial aftermarket). Step 5: Explicitly disregard UFL row 0f9c141a4623219910b7c3d78b3d3704 which incorrectly lists 2.52e+08 (this is the Cost of Sales increase, not Organic Sales, despite the label).\n",
      "\n",
      "=== FINAL AGENT RESPONSE ===\n",
      "ANSWER:    The sales excluding acquisition, or Organic Sales, increased by $615 million (7.7% increase) in 2025 compared to 2024, primarily due to increases in defense and commercial aftermarket (Source: a6ddc782-e158-45cf-9027-bddf4262cdde, 0dbf27f3-f0e1-4c5a-8a20-501138370fed, 314fe7fb-2dc7-4dd4-bbe8-828e174abb76).\n",
      "NUANCES:   The increase is primarily related to defense and commercial aftermarket, with detailed segment breakdowns showing increases in Power & Control and Airframe segments.\n",
      "CITATIONS: ['a6ddc782-e158-45cf-9027-bddf4262cdde', '0dbf27f3-f0e1-4c5a-8a20-501138370fed', '314fe7fb-2dc7-4dd4-bbe8-828e174abb76', '931d9581c4416e15b8ad17b1cc70d856']\n",
      "GROUNDED:  0.95\n"
     ]
    }
   ],
   "source": [
    "query = \"How much the sale excluding acquisition increase compare to last year and what it was due to?\"\n",
    "\n",
    "# 1. Navigate\n",
    "plan = await nav.navigate(query)\n",
    "\n",
    "# 2. Retrieve\n",
    "results = await retriever.retrieve(plan, k=3, include_all_chunks_for_ufl=True, include_all_ufl_for_chunks=True)\n",
    "\n",
    "# 3. Assemble\n",
    "context = assembler.assemble(results)\n",
    "\n",
    "# 4. Answer\n",
    "full_result = await agent.answer(query, context)\n",
    "response = full_result[\"final_response\"]\n",
    "reasoning = full_result[\"reasoning\"]\n",
    "code_result = full_result[\"code_result\"]\n",
    "\n",
    "print(\"=== AGENT THOUGHT PROCESS ===\")\n",
    "print(f\"PLAN: {reasoning.plan}\")\n",
    "if reasoning.requires_math:\n",
    "    print(f\"\\n--- EXECUTED PYTHON ---\")\n",
    "    print(reasoning.python_code)\n",
    "    print(f\"\\n--- CODE OUTPUT ---\")\n",
    "    print(code_result['output'] if code_result else 'None')\n",
    "\n",
    "print(\"\\n=== FINAL AGENT RESPONSE ===\")\n",
    "print(f\"ANSWER:    {response.answer}\")\n",
    "print(f\"NUANCES:   {response.nuances}\")\n",
    "print(f\"CITATIONS: {response.citations}\")\n",
    "print(f\"GROUNDED:  {response.groundedness_score}\")\n",
    "if response.is_self_aware_warning:\n",
    "    print(\"!!! WARNING: Agent is not fully confident in this answer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Missing Data Test\n",
    "\n",
    "\"What was the CEO's favorite color in 2025?\" (Should trigger self-awareness/not found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-03 17:40:04,187 - venra - INFO - Navigating query: What was the CEO's favorite color in 2025?\n",
      "2026-02-03 17:40:04,679 - venra - INFO - Plan generated. Reasoning: The question asks for a non-financial, non-quantitative piece of information about the CEO, which is not typically found in a 10-K document. This suggests that the answer may not be available in the provided schema.\n",
      "2026-02-03 17:40:04,680 - venra - INFO - Starting retrieval for query: CEO's favorite color in 2025... (k=2)\n",
      "2026-02-03 17:40:05,541 - venra - INFO - Retrieval complete: 7 UFL rows, 2 text chunks.\n",
      "2026-02-03 17:40:05,546 - venra - INFO - Agent processing query (Pass 1: Kimi Reasoning): What was the CEO's favorite color in 2025?\n",
      "2026-02-03 17:40:25,266 - venra - INFO - Reasoning Plan: 1. Search the UFL table for any metric related to 'CEO', 'favorite', or 'color' - no such entries found. 2. Search the Source Text Chunks (f1bb65a4-9938-465d-8a10-db1c7f16795f and 0e2eb9eb-0bce-4201-a212-1b4fc07e6d97) for any mention of CEO personal preferences or favorite colors - no such information found. 3. The provided context contains only financial metrics (Revenue, Interest Rate, Acquisitions, Divestitures, Legal Rulings, Risk Factors, Return) and stock performance data. 4. Conclude that the requested information is not present in the provided context and cannot be calculated from available data.\n",
      "2026-02-03 17:40:25,267 - venra - INFO - Pass 2: Synthesis (Fast)\n",
      "=== AGENT RESPONSE (MISSING DATA) ===\n",
      "ANSWER:    The CEO's favorite color in 2025 is not mentioned in the provided context.\n",
      "GROUNDED:  0.0\n"
     ]
    }
   ],
   "source": [
    "query = \"What was the CEO's favorite color in 2025?\"\n",
    "plan = await nav.navigate(query)\n",
    "results = await retriever.retrieve(plan, k=2)\n",
    "context = assembler.assemble(results)\n",
    "full_result = await agent.answer(query, context)\n",
    "response = full_result[\"final_response\"]\n",
    "\n",
    "print(\"=== AGENT RESPONSE (MISSING DATA) ===\")\n",
    "print(f\"ANSWER:    {response.answer}\")\n",
    "print(f\"GROUNDED:  {response.groundedness_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-03 17:52:47,704 - venra - INFO - Navigating query: How much the sale decrease overal compare to last year and why it happend?\n",
      "2026-02-03 17:52:48,266 - venra - INFO - Plan generated. Reasoning: The user is asking for the overall decrease in sales compared to last year and the reason behind it. This requires looking at the 'Net Sales' metric for both the current and prior years, and then finding the reason for the decrease.\n",
      "2026-02-03 17:52:48,267 - venra - INFO - Starting retrieval for query: Net Sales Decrease of 5.6% Compared to Last Year.... (k=3)\n",
      "2026-02-03 17:52:48,945 - venra - INFO - Keyword Boost Search: 'Net Sales Decrease Last Year Reason' (k=5)\n",
      "2026-02-03 17:52:49,699 - venra - INFO - Retrieval complete: 23 UFL rows, 5 text chunks.\n",
      "2026-02-03 17:52:49,717 - venra - INFO - Agent processing query (Pass 1: Kimi Reasoning): How much the sale decrease overal compare to last year and why it happend?\n",
      "2026-02-03 17:54:05,389 - venra - INFO - Reasoning Plan: 1. Identify the contradiction: The UFL shows Net Sales as 500M USD for both 2024 and 2025 (5e+08), but the Source Text Chunk (0dbf27f3-f0e1-4c5a-8a20-501138370fed) contains a detailed table showing Net Sales of $8,831 million for 2025 and $7,940 million for 2024. Per the rule to trust text over broken data, I will use the text values. 2. Calculate the actual change: The data shows an INCREASE of $891 million (from $7,940M to $8,831M), not a decrease. The percentage increase is 11.2%. 3. Decompose the change: The text attributes the growth to two components - Organic sales growth ($615M increase, 7.7%) and Acquisition sales growth ($276M increase, 3.5%). 4. Address the 'why': Note that while the numerical components are provided, the specific qualitative drivers (e.g., market conditions, pricing strategies) for the organic growth are not detailed in the provided text chunks. Also note that Selling and Administrative Expenses decreased by $35M, which may be the source of confusion in the query.\n",
      "2026-02-03 17:54:05,391 - venra - INFO - Executing Python code:\n",
      "sales_2025 = 8831\n",
      "sales_2024 = 7940\n",
      "dollar_change = sales_2025 - sales_2024\n",
      "pct_change = (dollar_change / sales_2024) * 100\n",
      "\n",
      "organic_2025 = 8510\n",
      "organic_2024 = 7895\n",
      "acq_2025 = 321\n",
      "acq_2024 = 45\n",
      "\n",
      "organic_change = organic_2025 - organic_2024\n",
      "acq_change = acq_2025 - acq_2024\n",
      "\n",
      "print(f\"Net Sales 2025: ${sales_2025} million\")\n",
      "print(f\"Net Sales 2024: ${sales_2024} million\")\n",
      "print(f\"Total Dollar Change: ${dollar_change} million (Increase)\")\n",
      "print(f\"Percentage Change: {pct_change:.1f}%\")\n",
      "print(f\"  - Organic Sales Contribution: ${organic_change} million\")\n",
      "print(f\"  - Acquisition Sales Contribution: ${acq_change} million\")\n",
      "print(f\"Verification: ${organic_change + acq_change} million = ${dollar_change} million\")\n",
      "2026-02-03 17:54:05,392 - venra - INFO - Code output: Net Sales 2025: $8831 million\n",
      "Net Sales 2024: $7940 million\n",
      "Total Dollar Change: $891 million (Increase)\n",
      "Percentage Change: 11.2%\n",
      "  - Organic Sales Contribution: $615 million\n",
      "  - Acquisition Sales Contribution: $276 million\n",
      "Verification: $891 million = $891 million\n",
      "2026-02-03 17:54:05,393 - venra - INFO - Pass 2: Synthesis (Fast)\n",
      "=== AGENT THOUGHT PROCESS ===\n",
      "PLAN: 1. Identify the contradiction: The UFL shows Net Sales as 500M USD for both 2024 and 2025 (5e+08), but the Source Text Chunk (0dbf27f3-f0e1-4c5a-8a20-501138370fed) contains a detailed table showing Net Sales of $8,831 million for 2025 and $7,940 million for 2024. Per the rule to trust text over broken data, I will use the text values. 2. Calculate the actual change: The data shows an INCREASE of $891 million (from $7,940M to $8,831M), not a decrease. The percentage increase is 11.2%. 3. Decompose the change: The text attributes the growth to two components - Organic sales growth ($615M increase, 7.7%) and Acquisition sales growth ($276M increase, 3.5%). 4. Address the 'why': Note that while the numerical components are provided, the specific qualitative drivers (e.g., market conditions, pricing strategies) for the organic growth are not detailed in the provided text chunks. Also note that Selling and Administrative Expenses decreased by $35M, which may be the source of confusion in the query.\n",
      "\n",
      "--- EXECUTED PYTHON ---\n",
      "sales_2025 = 8831\n",
      "sales_2024 = 7940\n",
      "dollar_change = sales_2025 - sales_2024\n",
      "pct_change = (dollar_change / sales_2024) * 100\n",
      "\n",
      "organic_2025 = 8510\n",
      "organic_2024 = 7895\n",
      "acq_2025 = 321\n",
      "acq_2024 = 45\n",
      "\n",
      "organic_change = organic_2025 - organic_2024\n",
      "acq_change = acq_2025 - acq_2024\n",
      "\n",
      "print(f\"Net Sales 2025: ${sales_2025} million\")\n",
      "print(f\"Net Sales 2024: ${sales_2024} million\")\n",
      "print(f\"Total Dollar Change: ${dollar_change} million (Increase)\")\n",
      "print(f\"Percentage Change: {pct_change:.1f}%\")\n",
      "print(f\"  - Organic Sales Contribution: ${organic_change} million\")\n",
      "print(f\"  - Acquisition Sales Contribution: ${acq_change} million\")\n",
      "print(f\"Verification: ${organic_change + acq_change} million = ${dollar_change} million\")\n",
      "\n",
      "--- CODE OUTPUT ---\n",
      "Net Sales 2025: $8831 million\n",
      "Net Sales 2024: $7940 million\n",
      "Total Dollar Change: $891 million (Increase)\n",
      "Percentage Change: 11.2%\n",
      "  - Organic Sales Contribution: $615 million\n",
      "  - Acquisition Sales Contribution: $276 million\n",
      "Verification: $891 million = $891 million\n",
      "\n",
      "=== FINAL AGENT RESPONSE ===\n",
      "ANSWER:    The sales did not decrease, but instead increased by $891 million, which is an 11.2% increase from $7,940 million in 2024 to $8,831 million in 2025 (Source: 0dbf27f3-f0e1-4c5a-8a20-501138370fed).\n",
      "NUANCES:   The increase is attributed to both organic sales growth of $615 million (7.7% increase) and acquisition sales growth of $276 million (3.5% increase).\n",
      "CITATIONS: ['0dbf27f3-f0e1-4c5a-8a20-501138370fed']\n",
      "GROUNDED:  0.9\n"
     ]
    }
   ],
   "source": [
    "query = \"How much the sale decrease overal compare to last year and why it happend?\"\n",
    "\n",
    "# 1. Navigate\n",
    "plan = await nav.navigate(query)\n",
    "\n",
    "# 2. Retrieve\n",
    "results = await retriever.retrieve(plan, k=3, include_all_chunks_for_ufl=True, include_all_ufl_for_chunks=True)\n",
    "\n",
    "# 3. Assemble\n",
    "context = assembler.assemble(results)\n",
    "\n",
    "# 4. Answer\n",
    "full_result = await agent.answer(query, context)\n",
    "response = full_result[\"final_response\"]\n",
    "reasoning = full_result[\"reasoning\"]\n",
    "code_result = full_result[\"code_result\"]\n",
    "\n",
    "print(\"=== AGENT THOUGHT PROCESS ===\")\n",
    "print(f\"PLAN: {reasoning.plan}\")\n",
    "if reasoning.requires_math:\n",
    "    print(f\"\\n--- EXECUTED PYTHON ---\")\n",
    "    print(reasoning.python_code)\n",
    "    print(f\"\\n--- CODE OUTPUT ---\")\n",
    "    print(code_result['output'] if code_result else 'None')\n",
    "\n",
    "print(\"\\n=== FINAL AGENT RESPONSE ===\")\n",
    "print(f\"ANSWER:    {response.answer}\")\n",
    "print(f\"NUANCES:   {response.nuances}\")\n",
    "print(f\"CITATIONS: {response.citations}\")\n",
    "print(f\"GROUNDED:  {response.groundedness_score}\")\n",
    "if response.is_self_aware_warning:\n",
    "    print(\"!!! WARNING: Agent is not fully confident in this answer.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
